{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the goal is two folds:\n",
    "1. Model the different covariates from the text only\n",
    "2. Then model the outcome given the predicted covariates & compare this with model build on the true covariates\n",
    "\n",
    "An important consideration is that we want the split to be the same across all notebooks, we save this information to be sure to be consistent across all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/projects/wangc/jeanselme/conda_env/' # BE SURE TO NOT SAVE in your home otherwise will be blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes  = pd.read_csv('data/TGCA_Merged.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_to_predict = outcomes[['type', 'gender', 'race', 'ajcc_pathologic_tumor_stage']]\n",
    "outcomes_to_predict['ajcc_pathologic_tumor_stage'] = outcomes_to_predict.ajcc_pathologic_tumor_stage.astype('category')\n",
    "outcomes_to_predict_dummy = pd.get_dummies(outcomes_to_predict, dummy_na = True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_to_predict_dummy.to_csv('data/binary_embedding.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose different evaluation procedures:\n",
    "1. One hospital out evaluation: to evaluate how well the model generalise outside the cohort. To limit the number of split, we compute only for hospitals with more than 100 patients.\n",
    "2. One cancer group out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = pd.DataFrame({\n",
    "        'Hospital': pd.factorize(outcomes.Hospital.replace({'Other': np.nan}))[0],\n",
    "        'Grouping' : pd.factorize(outcomes.grouping.replace({'Other': np.nan}))[0],\n",
    "    }, index = outcomes.index).replace({-1: np.nan})\n",
    "split.to_csv('results/split.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of the section Embedding, Fine-Tuning or Promptin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "From extracted embedding from LLM try to predict the different covariates of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding to use for the notebook\n",
    "embedding_type = 'BERT' # BERT, clinicalBERT, gpt, gpt+framing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = pd.read_csv('data/{}_embedding.csv'.format(embedding_type), index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (outcomes.index == embedding.index).all(), 'Misaligned index may create an issue - How is the embedding obtained?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model the different covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to predict from the text each manually extracted covariates. \n",
    "\n",
    "Then we save these covariates for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we rely on a NN from sklearn for this task\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for split_type in split.columns:\n",
    "    predictions[split_type] = pd.DataFrame().reindex_like(outcomes_to_predict_dummy)\n",
    "    for fold in split[split_type].dropna().unique():\n",
    "        train = split[split_type].values != fold\n",
    "        test = split[split_type].values == fold\n",
    "\n",
    "        model = MLPClassifier(hidden_layer_sizes = [], random_state = 42, \n",
    "                              learning_rate_init = 0.01, max_iter = 10, \n",
    "                              early_stopping = True).fit(embedding[train].values, outcomes_to_predict_dummy[train].values)\n",
    "        predictions[split_type][test] = model.predict_proba(embedding[test].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Go to Section 'Binarise and Save' to save this extraction.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine - Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful this will overwrite the previous Section (jump to last to save and evaluate).\n",
    "In this Section, we aim to fine-tune a neural network to extract the concept of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_type = 'BERT_finetune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available(), 'Machine or configuration not using GPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embedding):\n",
    "    if embedding == 'BERT_finetune':\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = outcomes_to_predict_dummy.shape[1],\n",
    "            output_attentions = False, output_hidden_states = False, problem_type=\"multi_label_classification\")\n",
    "        \n",
    "    return tokenizer, model\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.values\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx]).float()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = 'results/', num_train_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for split_type in split.columns:\n",
    "    predictions[split_type] = pd.DataFrame().reindex_like(outcomes_to_predict_dummy)\n",
    "    for fold in split[split_type].dropna().unique():\n",
    "        train = split[split_type].values != fold\n",
    "        test = split[split_type].values == fold\n",
    "\n",
    "        # Load model and encode data\n",
    "        tokenizer, model = get_model(embedding_type)\n",
    "        train_endcoded = Dataset(tokenizer(outcomes[train].text.tolist(), truncation = True, padding = True), outcomes_to_predict_dummy[train])\n",
    "        test_endcoded = Dataset(tokenizer(outcomes[test].text.tolist(), truncation = True, padding = True), outcomes_to_predict_dummy[test])\n",
    "\n",
    "        # Train model\n",
    "        trainer = Trainer(model = model, args = training_args, \n",
    "                          train_dataset = train_endcoded)\n",
    "        trainer.train()\n",
    "\n",
    "        # Predict\n",
    "        predictions[split_type][test] = trainer.predict(test_endcoded).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Go to Section 'Binarise and Save' to save this extraction.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline(\"text-generation\", model=\"medalpaca/medalpaca-7b\", tokenizer=\"medalpaca/medalpaca-7b\") # Should be downloaded in my folder (10 Gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_match = {\n",
    "    \"type\": (\"cancer_type\", outcomes_to_predict[\"type\"].dropna().unique()),\n",
    "    \"gender\": ('sex', ['male', 'female']),\n",
    "    \"race\": ('ethnicity', ['white', 'non-white']),\n",
    "    \"ajcc_pathologic_tumor_stage\": ('Cancer stage', ['I', 'II', 'III'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarised_predictions = pd.DataFrame().reindex_like(outcomes_to_predict)\n",
    "for outcome in outcomes_to_predict:\n",
    "    name, values = outcomes_match[outcome]\n",
    "    for patient in binarised_predictions.index:\n",
    "        patient_text = outcomes.text[patient]\n",
    "        prompt = \"What is the {} for a patient with the following pathology report: '{}'. Respond with exactly one of the following categories: {}.\".format(name, patient_text, values)\n",
    "        binarised_predictions.loc[patient, outcome] = model(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarised_predictions.to_csv('data/alpaca_predicted_binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"You are done ! This method does not require binarisations.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretise and save\n",
    "\n",
    "As the previous sections predict probabilities, we then discretise the outputs to match the concept of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarisation by a softmax\n",
    "binarised_predictions = []\n",
    "for column in outcomes_to_predict.columns:\n",
    "    if column == 'ajcc_pathologic_tumor_stage': # Special float case\n",
    "        pred_col = predictions.loc[:, predictions.columns.str.contains(column)].idxmax(axis = 1).str.replace(column + '_', '').astype(float)\n",
    "    elif column == 'type': # Special categorial case (change for other dataset)\n",
    "        pred_col = predictions.loc[:, predictions.columns.str.contains(column)].idxmax(axis = 1).str.replace(column + '_', '')\n",
    "    else: # All other binary variable\n",
    "        pred_col = predictions.loc[:, column] > 0.5\n",
    "\n",
    "    binarised_predictions.append(pred_col.rename(column))\n",
    "binarised_predictions = pd.concat(binarised_predictions, axis = 1)\n",
    "binarised_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarised_predictions.to_csv('data/{}_predicted_binary.csv'.format(embedding_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now saved the predicted binary outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure performance of the extraction\n",
    "\n",
    "This section allows you to check the performance for extracting the concetps of interest. Maybe the text does not contain the information or any indicator of the conctept you aim to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {}\n",
    "for split_type in split.columns:\n",
    "    columns = split[split_type].dropna().unique()\n",
    "    performance[split_type] = pd.DataFrame(index = columns, columns = predictions.columns)\n",
    "    for fold in columns:\n",
    "        for dimension in predictions.columns:\n",
    "            test = split[split_type] == fold\n",
    "            mean = outcomes_to_predict_dummy.loc[test, dimension].mean()\n",
    "            if mean != 1 and mean != 0:\n",
    "                # The class contains some positive\n",
    "                performance[split_type].loc[fold, dimension] = roc_auc_score(outcomes_to_predict_dummy.loc[test, dimension], predictions.loc[(split_type, test[test].index), dimension])\n",
    "performance = pd.concat(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.loc['Hospital'].astype(float).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.loc['Grouping'].astype(float).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
