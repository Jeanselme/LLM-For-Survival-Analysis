{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the goal is two folds:\n",
    "1. Model the different covariates from the text only\n",
    "2. Then model the outcome given the predicted covariates & compare this with model build on the true covariates\n",
    "\n",
    "An important consideration is that we want the split to be the same across all notebooks, we save this information to be sure to be consistent across all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open data and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding to use for the notebook\n",
    "embedding_type = 'BERT' # BERT, clinicalBERT, gpt, gpt+framing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = pd.read_csv('data/{}_embedding.csv'.format(embedding_type), index_col = 0)\n",
    "outcomes  = pd.read_csv('data/TGCA_Merged.csv', index_col = 0)\n",
    "embedding = embedding.loc[outcomes.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (outcomes.index == embedding.index).all(), 'Misaligned index may create an issue - How is the embedding obtained?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose different evaluation procedures:\n",
    "1. One hospital out evaluation: to evaluate how well the model generalise outside the cohort. To limit the number of split, we compute only for hospitals with more than 100 patients.\n",
    "2. One cancer group out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = pd.DataFrame({\n",
    "        'Hospital': pd.factorize(outcomes.Hospital.replace({'Other': np.nan}))[0],\n",
    "        'Grouping' : pd.factorize(outcomes.grouping.replace({'Other': np.nan}))[0],\n",
    "    }, index = outcomes.index).replace({-1: np.nan})\n",
    "split.to_csv('results/split.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model the different covariates and save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to predict from the text each manually extracted covariates. \n",
    "\n",
    "Then we save these covariates for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_to_predict = outcomes[['type', 'gender', 'race', 'ajcc_pathologic_tumor_stage']]\n",
    "outcomes_to_predict['ajcc_pathologic_tumor_stage'] = outcomes_to_predict.ajcc_pathologic_tumor_stage.astype('category')\n",
    "outcomes_to_predict_dummy = pd.get_dummies(outcomes_to_predict, dummy_na = True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_to_predict_dummy.to_csv('data/binary_embedding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we rely on a NN from sklearn for this task\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for split_type in split.columns:\n",
    "    predictions[split_type] = pd.DataFrame().reindex_like(outcomes_to_predict_dummy)\n",
    "    for fold in split[split_type].dropna().unique():\n",
    "        train = split[split_type].values != fold\n",
    "        test = split[split_type].values == fold\n",
    "\n",
    "        model = MLPClassifier(hidden_layer_sizes = [], random_state = 42, \n",
    "                              learning_rate_init = 0.01, max_iter = 10, \n",
    "                              early_stopping = True).fit(embedding[train].values, outcomes_to_predict_dummy[train].values)\n",
    "        predictions[split_type][test] = model.predict_proba(embedding[test].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarise and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarisation by a softmax\n",
    "binarised_predictions = []\n",
    "for column in outcomes_to_predict.columns:\n",
    "    if column == 'ajcc_pathologic_tumor_stage': \n",
    "        pred_col = predictions.loc[:, predictions.columns.str.contains(column)].idxmax(axis = 1).str.replace(column + '_', '').astype(float)\n",
    "    elif column == 'type':\n",
    "        pred_col = predictions.loc[:, predictions.columns.str.contains(column)].idxmax(axis = 1).str.replace(column + '_', '')\n",
    "    else:\n",
    "        pred_col = predictions.loc[:, column] > 0.5\n",
    "\n",
    "    binarised_predictions.append(pred_col.rename(column))\n",
    "binarised_predictions = pd.concat(binarised_predictions, axis = 1)\n",
    "binarised_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarised_predictions.to_csv('data/{}_predicted_binary.csv'.format(embedding_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure performance of the extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {}\n",
    "for split_type in split.columns:\n",
    "    columns = split[split_type].dropna().unique()\n",
    "    performance[split_type] = pd.DataFrame(index = columns, columns = predictions.columns)\n",
    "    for fold in columns:\n",
    "        for dimension in predictions.columns:\n",
    "            test = split[split_type] == fold\n",
    "            mean = outcomes_to_predict_dummy.loc[test, dimension].mean()\n",
    "            if mean != 1 and mean != 0:\n",
    "                # The class contains some positive\n",
    "                performance[split_type].loc[fold, dimension] = roc_auc_score(outcomes_to_predict_dummy.loc[test, dimension], predictions.loc[(split_type, test[test].index), dimension])\n",
    "performance = pd.concat(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.loc['Hospital'].astype(float).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.loc['Grouping'].astype(float).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
