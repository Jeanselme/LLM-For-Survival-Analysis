{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the goal is two folds:\n",
    "1. Model the different covariates from the text only\n",
    "2. Then model the outcome given the predicted covariates & compare this with model build on the true covariates\n",
    "\n",
    "An important consideration is that we want the split to be the same across all notebooks, we save this information to be sure to be consistent across all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes  = pd.read_csv('data/TGCA_Merged.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract quantiles for model prediction (and DeepHit discretisation)\n",
    "predictions_horizons = [1, 3, 5] # Time horizon in years (check that your data outcomes.t is in the same unit) - Limited time horizon for comparison with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reopen the split used for LLM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = pd.read_csv('results/split.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of the section Extracted Concepts, Embedding, Fine-Tuning or Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracted concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previously extracted concept, we aim to predict the survival outcome.\n",
    "1. Reopen the previous predictions\n",
    "2. Build a DeepHit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concepts used for predictions\n",
    "model_type= 'BERT' # BERT, clinicalBERT\n",
    "concept_type = 'embedding' # embedding, predicted_binary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = pd.read_csv('data/{}_{}.csv'.format(model_type, concept_type), index_col = [0, 1] if 'predicted_binary' in concept_type else [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (outcomes.index == concepts.index).all(), 'Misaligned index may create an issue - How is the embedding obtained?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepHit Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, t, e):\n",
    "        self.data = data.values\n",
    "        self.labels = (e.values, t.values)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"x\": torch.tensor(self.data[idx])}\n",
    "        item['labels'] = [torch.tensor(self.labels[0][idx]).float(), torch.tensor(self.labels[1][idx]).float()]\n",
    "        item['label_ids'] = [idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.deephit import DeepHitTorch\n",
    "from model.training import DeepHitTrainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = 'results/', num_train_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for split_type in split.columns:\n",
    "    predictions[split_type] = pd.DataFrame(index = outcomes.index, columns = predictions_horizons)\n",
    "    data = concepts.loc[(split_type,)] if concept_type == 'predicted_binary' else concepts\n",
    "    for fold in split[split_type].dropna().unique():\n",
    "        train = split[split_type].values != fold\n",
    "        test = split[split_type].values == fold\n",
    "\n",
    "        # Create Dataset Objects\n",
    "        train_encoded = Dataset(data[train], outcomes.t[train], outcomes.e[train])\n",
    "        test_encoded = Dataset(data[test], outcomes.t[test], outcomes.e[test])\n",
    "\n",
    "        # Train Model\n",
    "        model = DeepHitTorch(inputdim = data.shape[1], layers = [50, 50, 50], splits = predictions_horizons).double()\n",
    "        trainer = DeepHitTrainer(model, args = training_args, \n",
    "                                train_dataset = train_encoded)\n",
    "        trainer.train()\n",
    "\n",
    "        predictions[split_type][test] = trainer.predict(test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('results/{}_{}_predictions.csv'.format(model_type, concept_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine - Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful this will override the previous (jump to last to save and evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'BERT' # BERT, clinicalBERT\n",
    "model_type += '_finetune' # For naming convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from model.training import DeepHitTrainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available(), 'Machine or configuration not using GPU - This will be very slow.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embedding):\n",
    "    if embedding == 'BERT_finetune':\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = len(predictions_horizons),\n",
    "            output_attentions = False, output_hidden_states = False, problem_type=\"multi_label_classification\")\n",
    "    elif embedding == 'clinicalBERT_finetune':\n",
    "        from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('medicalai/ClinicalBERT')\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\"medicalai/ClinicalBERT\", num_labels = len(predictions_horizons),\n",
    "            output_attentions = False, output_hidden_states = False, problem_type=\"multi_label_classification\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, e, t):\n",
    "        self.encodings = encodings\n",
    "        self.labels = (e.values, t.values)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = [torch.tensor(self.labels[0][idx]).float(), torch.tensor(self.labels[1][idx]).float()]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = 'results/', num_train_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for split_type in split.columns:\n",
    "    predictions[split_type] = pd.DataFrame(index = outcomes.index, columns = predictions_horizons)\n",
    "    for fold in split[split_type].dropna().unique():\n",
    "        train = split[split_type].values != fold\n",
    "        test = split[split_type].values == fold\n",
    "\n",
    "        # Load model and encode data\n",
    "        tokenizer, model = get_model(model_type)\n",
    "        model.splits = predictions_horizons\n",
    "        train_encoded = Dataset(tokenizer(outcomes[train].text.tolist(), truncation = True, padding = True), outcomes.t[train], outcomes.e[train])\n",
    "        test_encoded = Dataset(tokenizer(outcomes[test].text.tolist(), truncation = True, padding = True), outcomes.t[test], outcomes.e[test])\n",
    "\n",
    "        # Train model\n",
    "        trainer = DeepHitTrainer(model = model, args = training_args, \n",
    "                          train_dataset = train_encoded)\n",
    "        trainer.train()\n",
    "\n",
    "        # Predict\n",
    "        predictions[split_type][test] = trainer.predict(test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('results/{}_predictions.csv'.format(model_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(llm):\n",
    "    if llm == 'llama':\n",
    "        model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    elif llm == 'medalpaca':\n",
    "        model = \"medalpaca/medalpaca-7b\"\n",
    "        return pipeline(\"text-generation\", model=model, tokenizer=model, max_length=1000)\n",
    "\n",
    "assert torch.cuda.is_available(), 'Machine or configuration not using GPU.'\n",
    "\n",
    "llm = 'medalpaca'\n",
    "pipeline = get_pipeline(llm)\n",
    "\n",
    "predictions = pd.DataFrame(index=outcomes.index, columns=[1, 3, 5])\n",
    "\n",
    "for patient in predictions.index:\n",
    "    patient_text = outcomes.text[patient]   \n",
    "    for horizon in predictions.columns:\n",
    "        context = f\"Pathology Report:\\n{patient_text}\"\n",
    "        question = f\"Based on the provided pathology report, what is the estimated probability (between 0 and 1) that the patient will die within the next {horizon} years? Please provide your answer as a single decimal number rounded to two decimal places, without any additional text or explanations.\"\n",
    "        \n",
    "        if llm == 'llama':\n",
    "            prompt = f\"\"\"<human>: Context: {context}Question: {question}Respond with only a float between 0 and 1, without any additional text.<assistant>:\"\"\"\n",
    "            inputs = pipeline.tokenizer(prompt, add_special_tokens=False, return_tensors=pipeline.framework)\n",
    "            #inputs = pipeline.tokenizer(prompt)\n",
    "            response = pipeline.model.generate(**inputs, max_length=1000, temperature=0.3, do_sample=False)[0]\n",
    "            response_text = pipeline.tokenizer.decode(response, skip_special_tokens=True)\n",
    "            response = response_text.split('<assistant>:')[-1].strip()\n",
    "            match = re.search(r'\\d+\\.\\d+', response)\n",
    "            if match:\n",
    "                value = match.group()\n",
    "                response = value\n",
    "            else:\n",
    "                print(\"No numeric value found.\")\n",
    "            #print(response)\n",
    "            \n",
    "        elif llm == 'medalpaca':\n",
    "            response = pipeline(f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\")[0]['generated_text'].split('Answer:')[-1].strip()\n",
    "        \n",
    "        #print(response)\n",
    "        predictions.loc[patient, horizon] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('results/{}_predictions.csv'.format(llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun these files with all the models you would like to use before evaluating using the `LLM.Analysis.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
