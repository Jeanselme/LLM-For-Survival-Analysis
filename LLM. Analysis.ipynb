{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to evaluate and compare the different models that are saved in `results/`. \n",
    "*It is to be used last.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pycox.evaluation import EvalSurv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate all models\n",
    "\n",
    "Enumerate all csv files and compare their performance on the different splits considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.read_csv('data/TGCA_Merged.csv', index_col = 0)\n",
    "split = pd.read_csv('results/split.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances = {}\n",
    "for file_name in sorted(os.listdir('results/')):\n",
    "    if 'predictions.csv' in file_name: \n",
    "        predictions = pd.read_csv('results/' + file_name, index_col = [0, 1])\n",
    "\n",
    "        model = file_name\n",
    "        model = model[:model.rindex('_')]\n",
    "        \n",
    "        performances[model] = {}\n",
    "        for split_type in split.columns:\n",
    "            columns = split[split_type].dropna().unique()\n",
    "            performances[model][split_type] = pd.DataFrame(index = ['C-Index', 'Brier'], columns = columns)\n",
    "            pred_split = predictions.loc[split_type]\n",
    "            pred_split.columns = pred_split.columns.astype(float)\n",
    "\n",
    "            for fold in columns:\n",
    "                train, test = split[split_type] != fold, split[split_type] == fold\n",
    "                train, test = train[train].index, test[test].index\n",
    "                ev = EvalSurv(pred_split.loc[test].T, outcomes.t.loc[test].values, outcomes.e.loc[test].values, censor_surv='km')\n",
    "                performances[model][split_type].loc['C-Index', fold] = ev.concordance_td()\n",
    "                performances[model][split_type].loc['Brier' , fold] = ev.integrated_brier_score(pred_split.columns.to_numpy())\n",
    "        performances[model] = pd.concat(performances[model])\n",
    "performances = pd.concat(performances)\n",
    "performances.index.set_names(['Model', 'Split', 'Metric'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
